{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flights Data Regression_PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCdiyhuHj17VLoPUu99MUt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nQbrTHxtrVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bde309-0b54-4924-fead-526f47e1c8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "kx4arDcDt2E9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "331b2c23-79a2-4f86-8414-766adad3b95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.0-bin-hadoop3.2'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "BcU8wLFQt7ji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "16263c77-19b3-4a90-f1e8-b9d687e382cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://55adfa2df95b:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc6c165c0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "ajmq8bAmuFdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from CSV file\n",
        "flights = spark.read.csv('/content/drive/MyDrive/Data/flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())\n",
        "\n",
        "# View the first five records\n",
        "flights.show(5)\n",
        "\n",
        "# Check column data types\n",
        "print(flights.dtypes)"
      ],
      "metadata": {
        "id": "Z1Mg1BW_uFoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae30df50-a65b-40e7-f310-342e086c333f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data contain 50000 records.\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHTD1dgMGczw"
      },
      "source": [
        "### Encoding flight origin\n",
        "\n",
        "The org column in the flights data is a categorical variable giving the airport from which a flight departs.\n",
        "\n",
        "* ORD — O'Hare International Airport (Chicago)\n",
        "* SFO — San Francisco International Airport\n",
        "* JFK — John F Kennedy International Airport (New York)\n",
        "* LGA — La Guardia Airport (New York)\n",
        "* SMF — Sacramento\n",
        "* SJC — San Jose\n",
        "* TUS — Tucson International Airport\n",
        "* OGG — Kahului (Hawaii)\n",
        "\n",
        "The data are in a variable called flights. You have already used a string indexer to create a column of indexed values corresponding to the strings in org."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights).transform(flights)\n",
        "\n",
        "flights.show(5)\n",
        "\n",
        "# Import the one hot encoder class\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
      ],
      "metadata": {
        "id": "P9DSPcfIuQyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79d3a17-8523-4814-aca5-35324a64537d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|    2.0|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|    0.0|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|    1.0|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|    0.0|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|    0.0|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+-------+-------------+\n",
            "|org|org_idx|    org_dummy|\n",
            "+---+-------+-------------+\n",
            "|ORD|    0.0|(7,[0],[1.0])|\n",
            "|SFO|    1.0|(7,[1],[1.0])|\n",
            "|JFK|    2.0|(7,[2],[1.0])|\n",
            "|LGA|    3.0|(7,[3],[1.0])|\n",
            "|SJC|    4.0|(7,[4],[1.0])|\n",
            "|SMF|    5.0|(7,[5],[1.0])|\n",
            "|TUS|    6.0|(7,[6],[1.0])|\n",
            "|OGG|    7.0|    (7,[],[])|\n",
            "+---+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Manupilation & Linear Regression"
      ],
      "metadata": {
        "id": "55ij7nkE9KcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required function\n",
        "from pyspark.sql.functions import round\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Read data from CSV file\n",
        "flights = spark.read.csv('/content/drive/MyDrive/Data/flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=['km'], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights = assembler.transform(flights)\n",
        "\n",
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 17)\n",
        "\n",
        "flights_train.show(5)\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a regression object and train on training data\n",
        "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "predictions = regression.transform(flights_test)\n",
        "predictions.select('duration', 'prediction').show(5, False)\n",
        "\n",
        "# Calculate the RMSE\n",
        "RegressionEvaluator(labelCol = 'duration').evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yBsqFbh81Fi",
        "outputId": "622bf9e6-0ead-4f33-b20b-e84b1f2b1032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
            "|mon|dom|dow|carrier|flight|org|depart|duration|delay|    km|features|\n",
            "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
            "|  0|  1|  2|     AA|    59|JFK|   7.0|     385|  -16|4162.0|[4162.0]|\n",
            "|  0|  1|  2|     AA|    73|ORD|  9.08|     560|   39|6828.0|[6828.0]|\n",
            "|  0|  1|  2|     AA|   150|SFO| 23.42|     325|   22|4352.0|[4352.0]|\n",
            "|  0|  1|  2|     AA|   154|ORD| 17.25|     135|   49|1395.0|[1395.0]|\n",
            "|  0|  1|  2|     AA|   181|JFK|  17.0|     379|  -10|3983.0|[3983.0]|\n",
            "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------+------------------+\n",
            "|duration|prediction        |\n",
            "+--------+------------------+\n",
            "|370     |345.9288489263871 |\n",
            "|310     |347.29222263984207|\n",
            "|115     |133.62126454782072|\n",
            "|80      |85.29724070425121 |\n",
            "|240     |213.53011275309635|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.07662043948225"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting Simple Model"
      ],
      "metadata": {
        "id": "UcE4xtGJ9rH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intercept (average minutes on ground)\n",
        "inter = regression.intercept\n",
        "print(inter)\n",
        "\n",
        "# Coefficients\n",
        "coefs = regression.coefficients\n",
        "print(coefs)\n",
        "\n",
        "# Average minutes per km\n",
        "minutes_per_km = regression.coefficients[0]\n",
        "print(minutes_per_km)\n",
        "\n",
        "# Average speed in km per hour\n",
        "avg_speed = 60 / minutes_per_km\n",
        "print(avg_speed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRt6sfht86kK",
        "outputId": "47eab05c-3bd3-4b2d-ff32-3d3b4f62137c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44.24454333244136\n",
            "[0.07574298408082997]\n",
            "0.07574298408082997\n",
            "792.152576613173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression"
      ],
      "metadata": {
        "id": "N8YD-7ey9wAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from CSV file\n",
        "flights = spark.read.csv('/content/drive/MyDrive/Data/flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "########## DEPART ############\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "# Create buckets at 3 hour intervals through the day\n",
        "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
        "\n",
        "# Bucket the departure times\n",
        "bucketed = buckets.transform(flights)\n",
        "bucketed.select('depart', 'depart_bucket')\n",
        "\n",
        "# Create a one-hot encoder\n",
        "onehot = OneHotEncoder(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
        "\n",
        "# One-hot encode the bucketed departure times\n",
        "flights = onehot.fit(bucketed).transform(bucketed)\n",
        "flights.select('depart', 'depart_bucket', 'depart_dummy')\n",
        "\n",
        "########## MILE ############\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
        "\n",
        "########## ORG ############\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights).transform(flights)\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx')\n",
        "\n",
        "########## MON ############\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights = StringIndexer(inputCol='mon', outputCol='mon_idx').fit(flights).transform(flights)\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoder(inputCols=['mon_idx'], outputCols=['mon_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights.select('mon', 'mon_idx', 'mon_dummy').distinct().sort('mon_idx')\n",
        "\n",
        "########## DOW ############\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights = StringIndexer(inputCol='dow', outputCol='dow_idx').fit(flights).transform(flights)\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoder(inputCols=['dow_idx'], outputCols=['dow_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights.select('dow', 'dow_idx', 'dow_dummy').distinct().sort('dow_idx')\n",
        "\n",
        "flights.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dOmrMoq9zmn",
        "outputId": "2f4ef7e0-a1b9-4f03-c3d2-4ffe532b12ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+---------------+-------+-------------+\n",
            "|mon|dom|dow|carrier|flight|org|depart|duration|delay|depart_bucket| depart_dummy|    km|org_idx|    org_dummy|mon_idx|      mon_dummy|dow_idx|    dow_dummy|\n",
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+---------------+-------+-------------+\n",
            "| 11| 20|  6|     US|    19|JFK|  9.48|     351| null|          3.0|(7,[3],[1.0])|3465.0|    2.0|(7,[2],[1.0])|    6.0| (11,[6],[1.0])|    6.0|    (6,[],[])|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|          5.0|(7,[5],[1.0])| 509.0|    0.0|(7,[0],[1.0])|    1.0| (11,[1],[1.0])|    2.0|(6,[2],[1.0])|\n",
            "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|          2.0|(7,[2],[1.0])| 542.0|    1.0|(7,[1],[1.0])|    4.0| (11,[4],[1.0])|    3.0|(6,[3],[1.0])|\n",
            "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|          3.0|(7,[3],[1.0])|1989.0|    0.0|(7,[0],[1.0])|   10.0|(11,[10],[1.0])|    1.0|(6,[1],[1.0])|\n",
            "|  4|  2|  5|     AA|   325|ORD|  8.92|      65| null|          2.0|(7,[2],[1.0])| 415.0|    0.0|(7,[0],[1.0])|    8.0| (11,[8],[1.0])|    0.0|(6,[0],[1.0])|\n",
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+---------------+-------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression : Train Test Split"
      ],
      "metadata": {
        "id": "ANsO4Ru396Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=['km','org_dummy','depart_dummy','dow_dummy','mon_dummy'], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights = assembler.transform(flights)\n",
        "\n",
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 17)\n",
        "\n",
        "flights_train.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NbtsPLE96nG",
        "outputId": "b8010895-e801-4188-866e-383a6fe62e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+--------------+-------+-------------+--------------------+\n",
            "|mon|dom|dow|carrier|flight|org|depart|duration|delay|depart_bucket| depart_dummy|    km|org_idx|    org_dummy|mon_idx|     mon_dummy|dow_idx|    dow_dummy|            features|\n",
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+--------------+-------+-------------+--------------------+\n",
            "|  0|  1|  2|     AA|    59|JFK|   7.0|     385|  -16|          2.0|(7,[2],[1.0])|4162.0|    2.0|(7,[2],[1.0])|    1.0|(11,[1],[1.0])|    2.0|(6,[2],[1.0])|(32,[0,3,10,17,22...|\n",
            "|  0|  1|  2|     AA|    73|ORD|  9.08|     560|   39|          3.0|(7,[3],[1.0])|6828.0|    0.0|(7,[0],[1.0])|    1.0|(11,[1],[1.0])|    2.0|(6,[2],[1.0])|(32,[0,1,11,17,22...|\n",
            "|  0|  1|  2|     AA|   150|SFO| 23.42|     325|   22|          7.0|    (7,[],[])|4352.0|    1.0|(7,[1],[1.0])|    1.0|(11,[1],[1.0])|    2.0|(6,[2],[1.0])|(32,[0,2,17,22],[...|\n",
            "|  0|  1|  2|     AA|   154|ORD| 17.25|     135|   49|          5.0|(7,[5],[1.0])|1395.0|    0.0|(7,[0],[1.0])|    1.0|(11,[1],[1.0])|    2.0|(6,[2],[1.0])|(32,[0,1,13,17,22...|\n",
            "|  0|  1|  2|     AA|   181|JFK|  17.0|     379|  -10|          5.0|(7,[5],[1.0])|3983.0|    2.0|(7,[2],[1.0])|    1.0|(11,[1],[1.0])|    2.0|(6,[2],[1.0])|(32,[0,3,13,17,22...|\n",
            "+---+---+---+-------+------+---+------+--------+-----+-------------+-------------+------+-------+-------------+-------+--------------+-------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression : Evaluating Model"
      ],
      "metadata": {
        "id": "E1zDvSBU-Lzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Fit linear regression model to training data\n",
        "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
        "\n",
        "# Make predictions on testing data\n",
        "predictions = regression.transform(flights_test)\n",
        "\n",
        "# Calculate the RMSE on testing data\n",
        "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
        "print(\"The test RMSE is\", rmse)\n",
        "\n",
        "# Look at the model coefficients\n",
        "coeffs = regression.coefficients\n",
        "print(coeffs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhHhebFa-Mtf",
        "outputId": "e73cb129-9236-49a2-d503-969507b0b63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test RMSE is 10.740502378624523\n",
            "[0.0744284401685256,26.996312105104003,19.94020032553227,51.76183943628122,45.45584247148976,17.25480651546479,14.855117875045766,16.800119688054526,-15.516868438024465,0.9376712941270354,3.9209465544338573,6.800981397048632,4.536562586593437,8.743745607877418,8.551832715240087,0.1399407485836122,0.08227287343586619,-0.12619923034732058,0.35431900572898783,0.4297011162497779,0.27432770544091817,-3.314805768755545,-1.1507984385493892,-3.6063230364191026,-1.4785231963021643,-1.353519806369124,-3.4516962545128136,0.8448188549985467,-2.724930340267113,-3.2054982432226864,-3.2564766790851403,-1.9150868176271394]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression : Regularization\n",
        "\n",
        "Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data.\n",
        "\n",
        "You'll use a specific value for the regularization strength. Later you'll learn how to find the best value using cross validation.\n",
        "\n",
        "The data (same as previous exercise) are available as flights, randomly split into flights_train and flights_test.\n",
        "\n",
        "For **Ridge**\n",
        "\n",
        "* LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)\n",
        "\n",
        "For **Lasso**\n",
        "\n",
        "* LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)"
      ],
      "metadata": {
        "id": "_4OGoypB-U18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Lasso model (α = 1) to training data\n",
        "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
        "\n",
        "# Calculate the RMSE on testing data\n",
        "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
        "print(\"The test RMSE is\", rmse)\n",
        "\n",
        "# Look at the model coefficients\n",
        "coeffs = regression.coefficients\n",
        "print(coeffs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rArqHUO-VVB",
        "outputId": "416eea78-4b9f-4f8c-993f-1a13bc4f01ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test RMSE is 11.749089740115453\n",
            "[0.07356941786680918,5.454471650523119,0.0,29.166723897433588,21.805904602903258,0.0,-2.340766656658043,0.0,0.0,0.0,0.0,0.0,0.0,1.1309839788797809,1.0811408456785048,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression Pipeline"
      ],
      "metadata": {
        "id": "uFthSwGiA0hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from CSV file\n",
        "flights = spark.read.csv('/content/drive/MyDrive/Data/flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
        "\n",
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 17)"
      ],
      "metadata": {
        "id": "OXarAjv5A45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = RegressionEvaluator(labelCol = 'duration')"
      ],
      "metadata": {
        "id": "MPW8F7pb_Eds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical strings to index values\n",
        "indexer = StringIndexer(inputCol = 'org', outputCol = 'org_idx')\n",
        "\n",
        "# One-hot encode index values\n",
        "onehot = OneHotEncoder(\n",
        "    inputCols=['org_idx','dow'],\n",
        "    outputCols=['org_dummy','dow_dummy']\n",
        ")\n",
        "\n",
        "# Assemble predictors into a single column\n",
        "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')\n",
        "\n",
        "# A linear regression object\n",
        "regression = LinearRegression(featuresCol='features', labelCol='duration')\n",
        "\n",
        "# Import class for creating a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Construct a pipeline\n",
        "pipeline = Pipeline(stages=[indexer,onehot, assembler, regression])\n",
        "\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "# Create parameter grid\n",
        "params = ParamGridBuilder()\n",
        "\n",
        "# Add grids for two parameters\n",
        "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
        "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "\n",
        "# Build parameter grid\n",
        "params = params.build()\n",
        "print('Number of models to be tested: ', len(params))\n",
        "\n",
        "# Create cross-validator\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# Train and test model on multiple folds of the training data\n",
        "cv = cv.fit(flights_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQoqLM7F_En6",
        "outputId": "8917e5bd-9f38-4316-a15d-84e97ee2dbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of models to be tested:  12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model from cross validation\n",
        "best_model = cv.bestModel\n",
        "\n",
        "# Look at the stages in the best model\n",
        "print(best_model.stages)\n",
        "\n",
        "##Prediction Evaluation\n",
        "evaluator.evaluate(best_model.transform(flights_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnyuby5HADUw",
        "outputId": "5c0857df-e99c-47a1-e105-8cee61c2c5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[StringIndexerModel: uid=StringIndexer_a8956839cb39, handleInvalid=error, OneHotEncoderModel: uid=OneHotEncoder_9a48040dddcc, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2, VectorAssembler_74f429109db2, LinearRegressionModel: uid=LinearRegression_a5660f6c037d, numFeatures=14]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.163705452721718"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}